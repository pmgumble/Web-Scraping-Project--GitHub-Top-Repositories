{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af5b41c",
   "metadata": {},
   "source": [
    "# Scraping the Top Repositories for topics on GitHub\n",
    "\n",
    "(Introduction):\n",
    "- **Introduction about webscraping**\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. It involves writing code to fetch web pages, parse their contents, and extract the desired information. Web scraping enables you to retrieve data from various online sources quickly and efficiently.\n",
    "\n",
    "Here's a brief overview of how web scraping works:\n",
    "\n",
    "1. Fetching web pages: Web scraping begins by sending HTTP requests to the target website's server to retrieve the HTML content of web pages. This can be done using various programming libraries or frameworks, such as Requests or Scrapy in Python.\n",
    "\n",
    "2. Parsing HTML: Once the HTML content is obtained, the next step is to parse it. Parsing involves analyzing the structure and elements of the HTML document to extract specific data. This is typically done using HTML parsing libraries like BeautifulSoup or lxml in Python.\n",
    "\n",
    "3. Navigating and locating elements: With the parsed HTML, you can navigate through the document's elements, such as tags, classes, or IDs, to locate the data you want to extract. You can use CSS selectors or XPath expressions to specify the elements of interest.\n",
    "\n",
    "4. Extracting data: Once the desired elements are located, you can extract the relevant data from them. This might involve retrieving text, attributes, or even URLs of images or links. The extracted data can be stored in variables, written to files, or processed further as needed.\n",
    "\n",
    "5. handling pagination and dynamic content: Many websites have multiple pages or load content dynamically through JavaScript. Web scraping may involve handling pagination by iterating through pages or using techniques like scrolling or interacting with APIs to access dynamic content.\n",
    "\n",
    "6. Data cleaning and processing: Extracted data often requires cleaning, formatting, and transformation to make it usable. This step involves removing unwanted characters, converting data types, performing calculations, or applying any necessary data manipulations.\n",
    "\n",
    "7. Storing or utilizing the data: The final step involves storing the extracted data in a structured format like CSV, JSON, or a database. Alternatively, you can directly utilize the scraped data for analysis, visualization, or integration with other applications.\n",
    "\n",
    "It's important to note that while web scraping can be a powerful tool for data collection, it's essential to comply with website terms of service, respect the website's robots.txt file (which specifies scraping rules), and not overload the target server with excessive requests.\n",
    "\n",
    "Additionally, some websites may have specific restrictions or employ measures like CAPTCHAs or IP blocking to prevent or deter web scraping. It's crucial to be aware of and respect these limitations while scraping data from websites.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Introduction about git hub**\n",
    "\n",
    "GitHub is a web-based platform and a widely used version control system that allows developers to collaborate on projects, track changes to their codebase, and manage code repositories. It provides a robust set of features and tools that facilitate code sharing, issue tracking, documentation, and project management. Here's an introduction to some key aspects of GitHub:\n",
    "\n",
    "1. Version Control: GitHub is built upon Git, a distributed version control system. Version control enables developers to track changes to their codebase over time, collaborate with others, and easily revert to previous versions if needed. Git's decentralized nature allows multiple developers to work on the same project concurrently, merging their changes seamlessly.\n",
    "\n",
    "2. Code Hosting and Collaboration: GitHub provides a platform for hosting Git repositories in the cloud. Developers can create repositories to store their code and share them with others. GitHub offers features like pull requests, which allow developers to propose changes, discuss them, and merge them into the main codebase. Collaboration on GitHub can happen within organizations, teams, or public open-source projects.\n",
    "\n",
    "3. Issue Tracking: GitHub includes an issue tracking system that helps manage tasks, bugs, and feature requests. Users can create issues, assign them to specific individuals or teams, add labels and milestones, and track the progress of each issue. This feature facilitates project management, communication, and coordination among team members.\n",
    "\n",
    "4. Documentation and Wikis: GitHub allows developers to create and maintain project documentation using built-in wikis or markdown files. This makes it easy to provide instructions, guidelines, and explanations for the codebase, enhancing collaboration and knowledge sharing within the project.\n",
    "\n",
    "5. Pull Requests and Code Reviews: Pull requests (PRs) are a fundamental feature of GitHub. They enable developers to propose changes to a repository and request that they be merged into the main codebase. Pull requests often include code diffs, comments, and discussions, allowing team members to review and provide feedback on the proposed changes before merging them.\n",
    "\n",
    "6. Integrations and Automation: GitHub supports integrations with various development tools and services. It offers a marketplace of applications and integrations that extend its functionality. Developers can automate workflows, perform continuous integration and deployment, and connect GitHub with tools like CI/CD systems, code quality analyzers, and project management platforms.\n",
    "\n",
    "7. Community and Open Source: GitHub has a vibrant community of developers, and it serves as a hub for open-source projects. Many projects on GitHub are openly available for anyone to contribute to, fostering collaboration, knowledge sharing, and the advancement of technology.\n",
    "\n",
    "8. GitHub provides an intuitive web interface, but it also offers a command-line interface (CLI) and can be integrated into development environments through various client applications and plugins. It has become an essential platform for developers to showcase their work, collaborate with others, and contribute to the open-source ecosystem.\n",
    "\n",
    "\n",
    "- **Problem statement**\n",
    "\n",
    "The task is to scrape the top repositories on GitHub and extract the topics associated with each repository using the BeautifulSoup and requests libraries in Python. The goal is to obtain the repository names, URLs, and their corresponding topics by parsing the HTML content of the GitHub trending page and the topics pages for each repository. The code should responsibly handle the scraping process by complying with website terms of service and avoiding excessive requests that could overload the server.\n",
    "\n",
    "\n",
    "- **Tools**\n",
    "(Python, Rquest, BeautifulSoup, pandas)\n",
    "\n",
    "\n",
    "**Note** - \n",
    "Web scraping should be done responsibly and in compliance with the website's terms of service. Make sure to add appropriate delays between requests to avoid overwhelming the server and potentially violating any usage limits or restrictions imposed by GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992215e5",
   "metadata": {},
   "source": [
    "## Here are the steps we are following \n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548273ba",
   "metadata": {},
   "source": [
    "# Scrape the list of topics from Github\n",
    "\n",
    "- Use requests to download the page\n",
    "- Use BS4 to parse and extract information \n",
    "- convert to pandas Dataframe \n",
    "\n",
    "Write a function to download the page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98b6c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    # This functio return BeutifoulSoup doc which conatains parsed HTML \n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    #Check succesfull response \n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to Load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0db539b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topics_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44c2d034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a87263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf02d4",
   "metadata": {},
   "source": [
    "### Lets create some helper function to parse information form the page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54db9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bb993",
   "metadata": {},
   "source": [
    "### `get_totpic_titles` can be used to get the list of titles \n",
    "\n",
    "to get topic titles, we can pick `p` tags with the `class` ...\n",
    "\n",
    "![](https://imgur.com/a/WeKTfYu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "325fa797",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_totpic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64c9b4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d77741b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122fd8b",
   "metadata": {},
   "source": [
    "### Similarly we have decribe the functions for descriptions and urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d277cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    topic_desc = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_desc.append(tag.text.strip())\n",
    "    return topic_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96cd4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940d9f6",
   "metadata": {},
   "source": [
    "Example and explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87d23f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    topic_links_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = \"https://github.com\"\n",
    "    for tag in topic_links_tags:\n",
    "        topic_urls.append(base_url+ tag['href'])\n",
    "    return topic_urls\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382407a",
   "metadata": {},
   "source": [
    "Lets put alttogether into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "405a8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    #Check succesfull response \n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to Load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics_dict = {\n",
    "        'title': get_totpic_titles(doc),\n",
    "        'description': get_totpic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6428b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) *1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ddbf7b",
   "metadata": {},
   "source": [
    "## Get the top 25 repositories  from the topic page \n",
    "\n",
    "Explanation and steps to follow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e99736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    # Download the page \n",
    "    response = requests.get(topic_url)\n",
    "    #Check succesfull response \n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to Load page {}'.format(topic_url))\n",
    "    # Parse using beautiful soup  \n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "039fe91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ad9f3",
   "metadata": {},
   "source": [
    "h1 tag with anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "089bd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    # return all the required information about a repo\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username =  a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, repo_url, stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a1daaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    # Get h3 tags containg repo title, repo url anmd username\n",
    "    h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h3_selection_class })\n",
    "    # Get star tags \n",
    "    star_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "    \n",
    "    topic_repos_dict = {\n",
    "    'usernames': [],\n",
    "    'repo_name': [],\n",
    "    'stars': [],\n",
    "    'repo_url': [] \n",
    "    }\n",
    "    \n",
    "    # Get Repository Information\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['usernames'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[3])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[2])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "46928da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "#     fname = topic_name + '.csv'\n",
    "    if os.path.exists(path):\n",
    "        print('The file {} already exists. Skipping..'.format(path))\n",
    "        return \n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(path + '.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2eed0",
   "metadata": {},
   "source": [
    "### Putting it all together \n",
    "\n",
    "- We have a function to get the list of topics \n",
    "- we have function to create a CSV file for scraped repos from topic page \n",
    "- Lets create a function to put them together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c2318205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics  ')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    # create folder here \n",
    "    os.makedirs('data', exist_ok = True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top Repositories for \"{}\" to'.format(row['title']))\n",
    "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0845e7",
   "metadata": {},
   "source": [
    "Lets run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d294ef",
   "metadata": {},
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032eb6a",
   "metadata": {},
   "source": [
    "We can check that CSVs are created properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19c454e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics  \n",
      "Scraping top Repositories for \"3D\" to\n",
      "Scraping top Repositories for \"Ajax\" to\n",
      "Scraping top Repositories for \"Algorithm\" to\n",
      "Scraping top Repositories for \"Amp\" to\n",
      "Scraping top Repositories for \"Android\" to\n",
      "Scraping top Repositories for \"Angular\" to\n",
      "Scraping top Repositories for \"Ansible\" to\n",
      "Scraping top Repositories for \"API\" to\n",
      "Scraping top Repositories for \"Arduino\" to\n",
      "Scraping top Repositories for \"ASP.NET\" to\n",
      "Scraping top Repositories for \"Atom\" to\n",
      "Scraping top Repositories for \"Awesome Lists\" to\n",
      "Scraping top Repositories for \"Amazon Web Services\" to\n",
      "Scraping top Repositories for \"Azure\" to\n",
      "Scraping top Repositories for \"Babel\" to\n",
      "Scraping top Repositories for \"Bash\" to\n",
      "Scraping top Repositories for \"Bitcoin\" to\n",
      "Scraping top Repositories for \"Bootstrap\" to\n",
      "Scraping top Repositories for \"Bot\" to\n",
      "Scraping top Repositories for \"C\" to\n",
      "Scraping top Repositories for \"Chrome\" to\n",
      "Scraping top Repositories for \"Chrome extension\" to\n",
      "Scraping top Repositories for \"Command line interface\" to\n",
      "Scraping top Repositories for \"Clojure\" to\n",
      "Scraping top Repositories for \"Code quality\" to\n",
      "Scraping top Repositories for \"Code review\" to\n",
      "Scraping top Repositories for \"Compiler\" to\n",
      "Scraping top Repositories for \"Continuous integration\" to\n",
      "Scraping top Repositories for \"COVID-19\" to\n",
      "Scraping top Repositories for \"C++\" to\n"
     ]
    }
   ],
   "source": [
    "# REad and display a csv using pandas \n",
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf08d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe4f440",
   "metadata": {},
   "source": [
    "## References and future Work\n",
    "\n",
    "Summary: \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
